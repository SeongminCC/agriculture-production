{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1JazBAUE9_TWC1wHmSGi5BO4GaO5BANXJ",
      "authorship_tag": "ABX9TyOuBiswj5jA202Rwe6rvqlC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SeongminCC/nongnet_AI/blob/main/modeling_fin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# validation AUC, MSE, RMSE, Accuracy"
      ],
      "metadata": {
        "id": "toOtZrY13B63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ewbALkKs_VRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "from glob import glob\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.layers import LSTM   # LSTM 층 \n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "import os\n",
        "\n",
        "import matplotlib.pylab as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import explained_variance_score, mean_squared_error, mean_absolute_error, r2_score, roc_auc_score\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import time\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "import copy\n",
        "from torchvision.utils import save_image\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "import math\n",
        "\n",
        "\n",
        "# 경고 끄기\n",
        "warnings.filterwarnings(action='ignore')\n",
        "\n",
        "# 시드고정\n",
        "tf.random.set_seed(19970119)\n",
        "random.seed(19970119)\n",
        "np.random.seed(19970119)\n",
        "\n",
        "\n",
        "\n",
        "%cd /content/drive/MyDrive/DNA/농넷_AIFactory/aT_data\n",
        "!ls -al\n",
        "\n",
        "df1 = pd.read_csv('./data/train/train_0.csv')\n",
        "\n",
        "\n",
        "df2 = df1.fillna(method='ffill') # 결측치 위치 기준 윗 값 가져오기 \n",
        "df2 = df1.fillna(method='bfill') # 결측치 위치 기준 아랫 값 가져오기\n",
        "\n",
        "\n",
        "date = pd.to_datetime(df1['datadate'], format='%Y%m%d')\n",
        "df2['datadate'] = date \n",
        "\n",
        "\n",
        "df3 = df2.iloc[:, :28]\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scale_cols = ['단가(원)', '거래량', '거래대금(원)', '경매건수', '도매시장코드', '도매법인코드', '산지코드 ', '해당일자_전체평균가격(원)', '해당일자_전체거래물량(kg)',\n",
        "              '하위가격 평균가(원)', '상위가격 평균가(원)', '하위가격 거래물량(kg)', '상위가격 거래물량(kg)', '일자별_도매가격_최대(원)',\n",
        "                '일자별_도매가격_평균(원)', '일자별_도매가격_최소(원)', '일자별_소매가격_최대(원)', '일자별_소매가격_평균(원)',\n",
        "                '일자별_소매가격_최소(원)', '수출중량(kg)', '수출금액(달러)', '수입중량(kg)', '수입금액(달러)',\n",
        "                '무역수지(달러)', '주산지_0_초기온도(℃)', '주산지_0_최대온도(℃)', '주산지_0_최저온도(℃)']\n",
        "                \n",
        "df_scaled = scaler.fit_transform(df3[scale_cols])\n",
        "\n",
        "df_scaled = pd.DataFrame(df_scaled)\n",
        "df_scaled.columns = scale_cols\n",
        "\n",
        "\n",
        "TEST_SIZE = 14  # 2주간 데이터 예측\n",
        "\n",
        "train = df_scaled[:-TEST_SIZE]\n",
        "test = df_scaled[-TEST_SIZE:]\n",
        "\n",
        "\n",
        "def make_dataset(data, label, window_size=4):\n",
        "    feature_list = []\n",
        "    label_list = []\n",
        "    for i in range(len(data) - window_size):\n",
        "        feature_list.append(np.array(data.iloc[i:i+window_size]))\n",
        "        label_list.append(np.array(label.iloc[i+window_size]))\n",
        "    return np.array(feature_list), np.array(label_list)\n",
        "\n",
        "\n",
        "feature_cols = ['단가(원)', '거래량', '거래대금(원)', '경매건수', '도매시장코드', '도매법인코드', '산지코드 ', '해당일자_전체거래물량(kg)',\n",
        "                '하위가격 평균가(원)', '상위가격 평균가(원)', '하위가격 거래물량(kg)', '상위가격 거래물량(kg)', '일자별_도매가격_최대(원)',\n",
        "                '일자별_도매가격_평균(원)', '일자별_도매가격_최소(원)', '일자별_소매가격_최대(원)', '일자별_소매가격_평균(원)',\n",
        "                '일자별_소매가격_최소(원)', '수출중량(kg)', '수출금액(달러)', '수입중량(kg)', '수입금액(달러)',\n",
        "                '무역수지(달러)', '주산지_0_초기온도(℃)', '주산지_0_최대온도(℃)', '주산지_0_최저온도(℃)']\n",
        "label_cols = ['해당일자_전체평균가격(원)']\n",
        "\n",
        "train_feature = train[feature_cols]\n",
        "train_label = train[label_cols]\n",
        "\n",
        "test_feature = test[feature_cols]\n",
        "test_label = test[label_cols]\n",
        "\n",
        "# train dataset\n",
        "train_feature, train_label = make_dataset(train_feature, train_label)\n",
        "\n",
        "# train, validation set 생성\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(train_feature, train_label, test_size=0.2)\n",
        "# ((1154, 4, 26), (289, 4, 26))\n",
        "\n",
        "# test dataset (실제 예측 해볼 데이터)\n",
        "test_feature, test_label = make_dataset(test_feature, test_label)\n",
        "# ((10, 4, 26), (10, 1))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "######################################################################################################################\n",
        "clf = RandomForestRegressor()\n",
        "\n",
        "test_size = 14\n",
        "X_train = df_scaled.drop(columns = '해당일자_전체평균가격(원)').iloc[:-test_size]\n",
        "y_train = df_scaled['해당일자_전체평균가격(원)'].iloc[:-test_size]\n",
        "\n",
        "X_test = df_scaled.drop(columns = '해당일자_전체평균가격(원)').iloc[-test_size:]\n",
        "y_test = df_scaled['해당일자_전체평균가격(원)'].iloc[-test_size:]\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "MSE = mean_squared_error(y_test, y_pred)\n",
        "AUC = roc_auc_score(y_test, y_pred)\n",
        "RMSE = math.sqrt(mean_squared_error(y_test, y_pred))\n",
        "Accuracy = clf.score(y_test, y_pred)\n",
        "\n",
        "print(MSE)\n",
        "print(AUC)\n",
        "print(RMSE)\n",
        "print(Accuracy)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "b96jteIb3NyT",
        "outputId": "a1d8977e-9a01-4d1a-9176-1197acd71561"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/DNA/농넷_AIFactory/aT_data\n",
            "total 5270\n",
            "-rw------- 1 root root   21765 Aug 28 08:31 answer_example.csv\n",
            "-rw------- 1 root root 4737091 Sep  5 06:04 aT_베이스라인.ipynb\n",
            "drwx------ 2 root root    4096 Sep  4 13:59 aT_test_raw\n",
            "drwx------ 2 root root    4096 Sep  4 13:59 aT_train_raw\n",
            "drwx------ 3 root root    4096 Sep  4 13:59 data\n",
            "-rw------- 1 root root  591766 Sep 12 13:47 modeling_2weeks.ipynb\n",
            "-rw------- 1 root root   16530 Sep 14 05:14 modeling_matlab.ipynb\n",
            "-rw------- 1 root root   15743 Aug 28 12:05 preprocessing.py\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-e946d4830418>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0mMSE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m \u001b[0mAUC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0mRMSE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0mAccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m             \u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m         )\n\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_base.py\u001b[0m in \u001b[0;36m_average_binary_score\u001b[0;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0my_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multilabel-indicator\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0} format is not supported\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: continuous format is not supported"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "writer_train = SummaryWriter(log_dir='/content/drive/MyDrive/DNA/농넷_AIFactory/aT_data/data/Log/AUC')\n",
        "writer_train = SummaryWriter(log_dir='/content/drive/MyDrive/DNA/농넷_AIFactory/aT_data/data/Log/Accuracy')\n",
        "writer_train = SummaryWriter(log_dir='/content/drive/MyDrive/DNA/농넷_AIFactory/aT_data/data/Log/MSE')\n",
        "writer_train = SummaryWriter(log_dir='/content/drive/MyDrive/DNA/농넷_AIFactory/aT_data/data/Log/RMSE')\n",
        "\n",
        "writer_train.add_scalar(\"MSE\", loss, epoch)"
      ],
      "metadata": {
        "id": "QLd65uJ586Ow"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}